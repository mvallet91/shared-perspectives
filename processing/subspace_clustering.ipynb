{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Subspace Clustering</h1>\n",
    "\n",
    "Subspace clustering attempts to find clusters in different subspaces of the dataset. Subspaces are projections of less dimensions than the full dataset, while they are part of it. For example, a line is a subspace of a plane.\n",
    "\n",
    "First thigs first, subspaces. Suppose that _V_ is a vector space and _W_ is a subset of _V_, _W_ ⊆ _V_. Endow _W_ with the same operations as _V_. Then _W_ is a subspace if and only if three conditions are met (read more at [Beezer](http://linear.ups.edu/html/fcla.html)).\n",
    "* _W_ is nonempty (_W_ ≠ ∅) and the zero vector belongs to _W_.\n",
    "* If _x_∈_W_ and _y_∈_W_, then (_x_ + _y_)∈_W_.\n",
    "* If α∈_C_ and _x_∈_W_, then α_x_∈_W_.\n",
    "\n",
    "On the other hand, the clustering algorithms are different than regular clustering in full space, they vary not only on how they classify instances but how they tackle the subspaces. There are two main reviews on Subspace Clustering (SSC), *Subspace Clustering for High Dimensional Data: A Review* and *Evaluating Clustering in Subspace Projections of High Dimensional Data*. \n",
    "\n",
    "The first one divides the algorithms in two main groups, Bottom Up vs Top Down, in a very broad sense this means: \n",
    "\n",
    "* Bottom Up method starts by creating clusters in the lowest dimensions and adding those relevant. It takes advantage of the downward closure property of density, which means that if there are dense units (clusters) in k dimensions, there are dense units in all (k − 1) dimensional projections. The examples are: CLIQUE, ENCLUS, MAFIA, CBF, CLTree and DOC.\n",
    "\n",
    "* Top Down finds clusters in the highest dimensional space, then iterates removing dimensions by their assigned weight. The exampels are: PROCLUS, ORCLUS, FINDIT and δ-Clusters.\n",
    "\n",
    "The second one focuses on what they call 3 paradigms of clustering: cell-based, density-based and cluster focused.\n",
    "\n",
    "* Cell-based: Sets of grid cells containg a certain amount of objects, such as CLIQUE, DOC, MINECLUS, and SCHISM \n",
    "* Density-based: Clusters are dense regions separated by sparse areas, such as SUBCLU, FIRES and INSCY\n",
    "* Cluster-oriented: Determine properties of the set of clusers, like number, size or dimentionality. They present PROCLUS, P3C and STATPC.\n",
    "\n",
    "There is an overlap in the following algorithms: PROCLUS (cluster-oriented/top-down), CLIQUE and DOC (grid-based/bottom-up). Further analysis on the algorithms will be done later, when required for implementation.\n",
    "\n",
    "<h2>OpenSubspace</h2>\n",
    "\n",
    "There is no Python implementation of SSC, and creating our own may be out of scope, especially since we are not sure it will work. So we will turn to an impementation in Weka (Waikato Environment for Knowledge Analysis, a machine learning suite) called  [OpenSubspace](http://dme.rwth-aachen.de/en/OpenSubspace) by the Data Management and Data Exploration Group of RWTH University. This implementation is based on the paper by Müller et al., *Evaluating Clustering in Subspace Projections of High Dimensional Data* so we focus on those available algorithms, especially those that overlap with the other review, adding FIRES because it is density based (and would be bottom-up) and the algorithm was provided by the original creators, while the rest are adapted for this implementation.\n",
    "\n",
    "For this we will need to transform our data to Attribute-Relation File Format (`.arff`, the native Weka format) files, and run the algorithms in Java via command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `arff.dump(object, file)` command takes a dictionary type object and encodes it into the required format. We have the 'Inverted Sentences' dictionary, but the arff structure is different, it looks like this:\n",
    " \n",
    "<pre><code>@RELATION rev_vectors\n",
    "\n",
    "@ATTRIBUTE dim_0 REAL\n",
    "@ATTRIBUTE dim_1 REAL\n",
    "@ATTRIBUTE \n",
    "@ATTRIBUTE dim_98 REAL\n",
    "@ATTRIBUTE dim_99 REAL\n",
    "\n",
    "@DATA\n",
    "-7.75659233e-02, 4.02860008e-02, ..., 4.38096404e-01, -3.18264008e-01 (this for the first vector)\n",
    "0.34120786, -0.23940383, ..., -0.45488459 -0.22274736\n",
    "...\n",
    "0.30884686, 0.4267047, ..., 0.84552544, -0.01002961\n",
    "0.78674525 -0.14647771, ..., 0.07416188, -0.80752152 (this for the last vector, number 155847)\n",
    "</pre></code>\n",
    "\n",
    "Where the ATTRIBUTES will be the dimensions of our vectors, like columns in a relation, named something like dim_0, dim_1, ..., dim_99; and in DATA, it is all the values for the row in that column.\n",
    "\n",
    "In dictionary type for Python, it should be like this:\n",
    "\n",
    "<pre><code>\n",
    "{u'attributes': [(u'dim_0', u'REAL'),\n",
    "             (u'dim_1', u'REAL'),\n",
    "             (u'...', u'REAL'),\n",
    "             (u'dim_98', u'REAL'),\n",
    "             (u'dim_99', u'REAL')],\n",
    " u'data': [[-7.75659233e-02, 4.02860008e-02, ..., 4.38096404e-01, -3.18264008e-01],\n",
    "           [0.34120786, -0.23940383, ..., -0.45488459 -0.22274736],\n",
    "           [...],\n",
    "           [0.30884686, 0.4267047, ..., 0.84552544, -0.01002961],\n",
    "           [0.78674525 -0.14647771, ..., 0.07416188, -0.80752152]],\n",
    " u'description': u'',\n",
    " u'relation': u'rev_vectors'}\n",
    " </pre></code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_arff(labeled_sentences, model, name):\n",
    "    \"\"\"\n",
    "    (LabeledSentences, doc2vec model, string) -> dict\n",
    "    \n",
    "    Requirements: \n",
    "    -All vectors are the same size\n",
    "    \n",
    "    Returns a dictionary ready for arff encoding.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vectors_arff = {}\n",
    "    attributes = []\n",
    "    data_arff = []\n",
    "    \n",
    "    for attribute in range(len(model.docvecs[sentences[0][1][0]])):\n",
    "        attributes.append(tuple(('dim_{}'.format(str(attribute)), 'REAL')))\n",
    "        \n",
    "    for x in range(len(sentences)-1):\n",
    "        vec = []\n",
    "        vec = model.docvecs[sentences[x][1][0]]\n",
    "        vec_list = vec.tolist()\n",
    "        data_arff.append(vec_list)\n",
    "    \n",
    "    vectors_arff['relation'] = name\n",
    "    vectors_arff['attributes'] = attributes\n",
    "    vectors_arff['data'] = data_arff\n",
    "    \n",
    "    return vectors_arff\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dictionary with the required arff structure using the desired corpus (in a list of LabeledSentence type) and the model, with a relation name. Then we encode it as an arff file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_arff = vector_arff(sentences, model, 'movies_amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arff.dump(movie_arff, open('movies.arff', 'w'))\n",
    "\n",
    "movie_arff = arff.load(open('movies.arff', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_command(command):\n",
    "    p = subprocess.Popen(command,\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.STDOUT)\n",
    "    return iter(p.stdout.readline, b'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for output_line in run_command(\"java -Xmx1024m -cp OpenSubspace\\* weka.subspaceClusterer.Proclus -t OpenSubspace\\data\\Databases\\synth_dbsizescale\\S1500.arff\"):\n",
    "    output.append(output_line.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to run the Subspace Clustering algorithm from here to the command line looks like this:\n",
    "\n",
    "<code>subprocess.check_output(['java', '-Xmx:memory', '-cp', 'jar location', 'algorithm', -t', 'arff file', 'cluster args'])</code>\n",
    "\n",
    "Some algorithm examples, with parameters and defaults:\n",
    "\n",
    "* *PROCLUS* -K (4), -D (3): In PROCLUS, K and D are pretty self-explanatory, they mean the amount of clusters to find, as well as the average dimentionality of the clusters. It's a fairly quick algorithm, but it requires some knowledge or restriction of the clusters needed.\n",
    "\n",
    "* *CLIQUE* -XI (10), -TAU (1.0): CLIQUE defines a cluster as a connection of grid cells with each more than τ (TAU) many objects. Grid cells are defined by a fixed grid splitting each dimension in ξ (XI) equal width cells.\n",
    "\n",
    "* *MINECLUS* -a (0.08), -b (0.25), -w (0.2), -m (-1), -n (1), -k (5): MINECLUS is an optimization of DOC by using Frequent Pattern trees to improve decision time and accuracy, which is an optimization of CLIQUE itself, by using flexible hypercubes of width _W_ instead of a fixed grid. α (0,1] is the minimun density of the discovered clusters, β (0,1] is a balance condition between the number of points and number of dimensions in a cluster, the measure  M is the MAXOUT parameter, n is the number of bins and k ???\n",
    "\n",
    "To work with several parameters, we can run experiments specifying the algorithm and tuning the values as necessary. It may not be intuitive and small changes can mean a large difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = [3]\n",
    "d = [5,6]\n",
    "proclus_experiments = {}\n",
    "algorithm = 'Proclus'\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import time\n",
    "for i in k:\n",
    "    for j in d:\n",
    "        start = time.time()\n",
    "        results = []\n",
    "        p = Popen(['java', '-Xmx2048m', '-cp', 'OpenSubspace\\*', 'weka.subspaceClusterer.{0}'.format(algorithm),\n",
    "                   '-t', 'pet_rev.arff', '-K', str(i), '-D', str(j) ], \n",
    "                  stdout=PIPE, stderr=STDOUT)\n",
    "        for line in p.stdout:\n",
    "            results.append(line.decode())\n",
    "        proclus_experiments[\"proclus_S1500_k{0}_d{1}\".format(i,j)] = results\n",
    "        end = time.time()\n",
    "        print('Done {0}_k{1}_d{2}'.format(algorithm,i,j), 'with clusters=', i, ', dimensions=', j, ' in ', (end-start)//60 , ' min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Proclus_Experiment(clusters, dimensions, dictionary, dataset):\n",
    "    algorithm = 'Proclus'    \n",
    "    from subprocess import Popen, PIPE, STDOUT\n",
    "    for i in clusters:\n",
    "        for j in dimensions:\n",
    "            start = time.time()\n",
    "            results = []\n",
    "            p = Popen(['java', '-Xmx4g', '-cp', 'OpenSubspace\\*', \n",
    "                       'weka.subspaceClusterer.{0}'.format(algorithm), \n",
    "                       '-t', 'OpenSubspace\\data\\Databases\\{}'.format(dataset), \n",
    "                       '-K', str(i), '-D', str(j) ], \n",
    "                       stdout=PIPE, stderr=STDOUT)\n",
    "            for line in p.stdout:\n",
    "                results.append(line.decode())\n",
    "            dictionary[\"exp_k{0}_d{1}\".format(i,j)] = results\n",
    "            end = time.time()\n",
    "            print('Done {0}_k{1}_d{2}'.format(algorithm,i,j), 'with clusters=', i, \n",
    "                  ', dimensions=', j, ' in ', (end-start), ' sec')\n",
    "            \n",
    "def Mineclus_Experiment(ALPHA, BETA, MAXOUT, K, numBins, W, dictionary, dataset):    \n",
    "    mineclus_exp_pet = {}\n",
    "    algorithm = 'MineClus'\n",
    "    for i in ALPHA:\n",
    "        for j in BETA:\n",
    "            for k in MAXOUT:\n",
    "                for l in K:\n",
    "                    for m in numBins:\n",
    "                        for n in W:\n",
    "                            start = time.time()\n",
    "                            results = []\n",
    "                            p = Popen(['java', '-Xmx4g', '-cp',  'OpenSubspace\\*', \n",
    "                                       'weka/subspaceClusterer/{0}'.format(algorithm),\n",
    "                                       '-t', 'OpenSubspace\\data\\Databases\\{}'.format(dataset), \n",
    "                                       '-a', str(i), '-b', str(j), '-m', str(k), \n",
    "                                       '-k', str(l), '-n', str(m), '-w', str(n)], \n",
    "                                      stdout=PIPE, stderr=STDOUT)\n",
    "                            for line in p.stdout:\n",
    "                                results.append(line.decode())\n",
    "                            dictionary[\"exp_a{}_b{}_m{}_k{}_n{}_w{}\".format(i,j,k,l,m,n)] = results\n",
    "                            end = time.time()\n",
    "                            print('Done {}_a{}_b{}_m{}_k{}_n{}_w{}'.format(algorithm,i,j,k,l,m,n), ' in ', (end-start), ' sec')\n",
    "\n",
    "def clusters_detail(experiment):\n",
    "    print(experiment[0], 'Number of Clusters', len(experiment)-3)\n",
    "    for i in range(len(experiment)):\n",
    "        s = experiment[i]\n",
    "        if '[' in s:\n",
    "            print('# instances in cluster {}:'.format(i-2), s[(s.index(']')+3):(s.index('{'))])\n",
    "            print('dimensions of clusters:', s[(s.index('[')):(s.index(']')+1)])\n",
    "    print('-'*100)\n",
    "    \n",
    "def clusters_info(experiment):\n",
    "    print(experiment[0], 'Number of Clusters', len(experiment)-3)\n",
    "    dims = []\n",
    "    for i in range(len(experiment)):\n",
    "        s = experiment[i]\n",
    "        if '[' in s:\n",
    "            #print('# instances in cluster {}:'.format(i-2), s[(s.index(']')+3):(s.index('{'))])\n",
    "            dims.append(int((s[(s.index('[')):(s.index(']')+1)]).count('1')))\n",
    "            #dims.append('1')\n",
    "            #print(int((s[(s.index('[')):(s.index(']')+1)]).count('1')))\n",
    "    if dims: print('Average dimensions =', str(round(np.mean(dims), 2)))   \n",
    "    #print('dims=',dims)\n",
    "    print('-'*100)\n",
    "\n",
    "def sample_from_cluster(experiment, cluster, size):\n",
    "    s = experiment[cluster+2]\n",
    "    values = s[(s.index('{'))+1:(s.index('}')-1)].split(' ')\n",
    "    sample = []\n",
    "    if size > 0:\n",
    "        for i in range(size): sample.append(int(random.choice(values)))\n",
    "    else:\n",
    "        for i in range(10): sample.append(int(random.choice(values)))\n",
    "    return sample\n",
    "\n",
    "def sample_sentences_from_cluster(experiment, cluster, size):\n",
    "    s = experiment[cluster+2]\n",
    "    values = s[(s.index('{'))+1:(s.index('}')-1)].split(' ')\n",
    "    sample = []\n",
    "    if size > 0:\n",
    "        for i in range(size): \n",
    "            rdm = int(random.choice(values))\n",
    "            sample.append(['Sentence #{}:'.format(i+1), sentences[rdm][1][0], ' '.join(sentences[rdm][0])])\n",
    "    else:\n",
    "        for i in range(10): sample.append('Sentence #{}:'.format(i+1) ,' '.join(sentences[int(random.choice(values))][0]))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = [3]\n",
    "d = [10, 20]\n",
    "proclus_test = {}\n",
    "Proclus_Experiment(k, d, proclus_test, 'movies.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are welcome to experiment with this, however we decided not to pursue the use of SSC because of the Extraction Method used in this work. The tuples resulting from doc2vec work best when all the dimensions are taken in account, so the effort on trying to get the most relevant dimensions is not so important. With some ideal algorithm that could name dimensions like \"funinnes\", \"educational level\", or \"intensity of acting\", then it may be interesting to look at the relevant dimensions for every movie.\n",
    "\n",
    "In addition, it does not work well with cosine distances, we tried!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
