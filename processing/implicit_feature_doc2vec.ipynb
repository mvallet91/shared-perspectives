{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Implicit Feature Extraction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the process of extraction for implicit features, using doc2vec by Gensim.\n",
    "(check here for more info: https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "\n",
    "The goal is to obtain the Perceptual Tuple from every review for the Experience Items available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Doc2vec</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we go through the process of training and testing a doc2vec model from Amazon User Reviews. The goal is to create high-dimensional vectors with latent features of the reviews. Doc2vec is a neural network that creates document (review) embeddings in a vector space. This is a high dimensional space, for example 100 dimensions. These vectors will later be used to try and cluster the reviews by the features the users care about and how they write about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.doc2vec\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Pre-processing </H2>\n",
    "\n",
    "The data was obtained as a json file of reviews from the UCSD website, http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews in dataset:  1697533\n",
      "Total movies and TV items in dataset:  50052\n"
     ]
    }
   ],
   "source": [
    "full_reviews = []\n",
    "with gzip.open('data/reviews_Movies_and_TV_5.json.gz') as f:\n",
    "    for line in f:\n",
    "        full_reviews.append(json.loads(line))\n",
    "print('Total reviews in dataset: ', len(full_reviews))\n",
    "\n",
    "items = defaultdict(int)\n",
    " \n",
    "for review in full_reviews:\n",
    "    asin = review['asin']\n",
    "    items[asin] += 1\n",
    "\n",
    "print('Total movies and TV items in dataset: ', len(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1.6 million reviews for around 50K movies and TV items from Amazon, and they look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asin': '0005019281',\n",
       " 'helpful': [0, 0],\n",
       " 'overall': 4.0,\n",
       " 'reviewText': 'This is a charming version of the classic Dicken\\'s tale.  Henry Winkler makes a good showing as the \"Scrooge\" character.  Even though you know what will happen this version has enough of a change to make it better that average.  If you love A Christmas Carol in any version, then you will love this.',\n",
       " 'reviewTime': '02 26, 2008',\n",
       " 'reviewerID': 'ADZPIG9QOCDG5',\n",
       " 'reviewerName': 'Alice L. Larson \"alice-loves-books\"',\n",
       " 'summary': 'good version of a classic',\n",
       " 'unixReviewTime': 1203984000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_200 = []\n",
    "for key in items:\n",
    "    if items[key] > 100 and items[key] < 300:\n",
    "        if len(movies_200) < 200:\n",
    "            movies_200.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use only the first 200 movies that have between 100 and 300 reviews, for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 31217 reviews, from 200 items\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for review in full_reviews:\n",
    "    if review['asin'] in movies_200:\n",
    "        reviews.append(review)\n",
    "        \n",
    "print('Now we have', len(reviews), 'reviews, from', len(movies_200), 'items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the reviews have to be transformed into a 'TaggedDocument' format consisting of unicode separate words (which form the \"documents\" of doc2vec) and a tag. In this case, the tag is created by concatenating the reviewer id with the product id from Amazon, such as 'A3UF8X1S0ZZ8KR|B000WUVZCK'. This tag has no effect for our training purpose. The text is tokenized and we only take in account reviews with more than 25 words, since Amazon requires 20 words, a lot of the ones that are barely long enough are not very useful for our purposes, for example: \"Great product a a a a a\"...until 25 words \"Came in time! f g h i j k \"...you get the idea.\n",
    "\n",
    "Here we also create a dictionary with the labels as keys and text as value, for easier reading and qualitative analysis later.\n",
    "\n",
    "It may be necessary to download the nltk tokenizer first, execute the download command and follow the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences introduced:  28944 for 28944 reviews\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "review_text = {}\n",
    "\n",
    "for review in reviews:\n",
    "    review_id = (review['reviewerID']+'|'+review['asin'])\n",
    "    sentence = models.doc2vec.LabeledSentence(\n",
    "    words = word_tokenize(review['reviewText'].lower()),\n",
    "    tags = [review_id])\n",
    "    if len(sentence[0]) > 25:\n",
    "        sentences.append(sentence)\n",
    "        review_text[review_id] = review['reviewText']\n",
    "        \n",
    "print('Total sentences introduced: ', len(sentences), 'for', len(review_text), 'reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a Labeled Sentence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledSentence(words=['this', 'is', 'a', 'charming', 'version', 'of', 'the', 'classic', 'dicken', \"'s\", 'tale', '.', 'henry', 'winkler', 'makes', 'a', 'good', 'showing', 'as', 'the', '``', 'scrooge', \"''\", 'character', '.', 'even', 'though', 'you', 'know', 'what', 'will', 'happen', 'this', 'version', 'has', 'enough', 'of', 'a', 'change', 'to', 'make', 'it', 'better', 'that', 'average', '.', 'if', 'you', 'love', 'a', 'christmas', 'carol', 'in', 'any', 'version', ',', 'then', 'you', 'will', 'love', 'this', '.'], tags=['ADZPIG9QOCDG5|0005019281'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If models are already available, skip to model loading. If not, continue here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Training </H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the model with parameters tuned by trial and error, initially based on the inforamtion established in the paper of the experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), and an example by gensim on IMDB [doc2vec & IMDB](http://localhost:8888/notebooks/GitHub/gensim/docs/notebooks/doc2vec-IMDB.ipynb):\n",
    "\n",
    "* `size` of 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
    "* The `window` is kept at 10 since it showed good performance with documents of similar size\n",
    "* Similarly, frequent word subsampling (restricting the amount of times that words can appear) seems to decrease sentiment-prediction accuracy, so it's left out.\n",
    "* `dm=0` means 'skip-gram' (PV-DBOW) mode, a distributed bag of words implementation, proven to be significantly faster and as accurate as the Distributed Memory (DM) mode.\n",
    "* A `min_count=5` saves quite a bit of model memory, discarding words that only appear five times or less, since they are not useful for our Shared Perspective concepts.\n",
    "* More `workers` allow faster processing when possibe\n",
    "* `alpha` is the initial learning rate, and will decrease linearly to min_alpha. In this case we keep it fixed to avoid decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc5,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "model = models.Doc2Vec(sentences, dm=0, size=100, window=10, min_count=5, workers=multiprocessing.cpu_count())\n",
    "\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be sure there are no overlapping tags, here we check whether the amount of created vectors us the same as the amount of documents put into each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(model.docvecs) == len(sentences), \"there are overlapping section titles! {0} docvecs and {1} documents\".format(len(model.docvecs), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the dbow model for a set number of iterations, for example 10. For more detail of hyper-parameters visit the following website: https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/doc2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41081398"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save (or load) the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"movies_dbow10epoch.doc2vec\")\n",
    "\n",
    "#model = Doc2Vec.load(\"movies_dbow10epoch.doc2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Analysis </H2>\n",
    "\n",
    "A.K.A. Playing with the model\n",
    "\n",
    "First of all, let's see the vectors obtained from the model, or the *Perceptual Tuples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = random.choice(list(review_text.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31564757, -0.25758761,  0.32343212,  0.11199896, -0.36150137,\n",
       "       -0.30959255, -0.11408874, -0.23506793,  0.15536129,  0.00984454,\n",
       "       -0.34564993, -0.64718872,  0.00950273,  0.18111324, -0.27767479,\n",
       "       -0.07515359,  0.32019427, -0.32216209, -0.224976  ,  0.23496059,\n",
       "        0.75499338,  0.13461852, -0.21235451,  0.4820191 , -0.10561399,\n",
       "       -0.11321183,  0.40781179, -0.46380854, -0.15351984, -0.07887659,\n",
       "       -0.4131723 ,  0.81180853,  0.00272181, -0.13923369, -0.08895218,\n",
       "        0.82202286, -0.08987688,  0.20316355, -0.18511321,  0.11422019,\n",
       "        0.49718273,  0.5551607 ,  0.06297428, -0.15935303, -0.8181302 ,\n",
       "        0.15749055,  0.24855907,  0.27744496, -0.39455798, -0.72817761,\n",
       "       -0.05193769, -0.10428967,  0.1791677 ,  0.39620438,  0.33176005,\n",
       "        0.26038209, -0.08656421,  0.4592309 , -0.28824422, -0.07417981,\n",
       "       -0.04835309, -0.09215254, -0.16211525,  0.50178498,  0.28218919,\n",
       "        0.15133481,  1.07339537,  0.45725924,  0.00947233,  0.16405132,\n",
       "       -0.03452886,  0.39162374, -0.61925799,  0.32119903,  0.17054465,\n",
       "       -0.27097315, -0.04140751, -0.43595779,  0.53771359,  0.31243902,\n",
       "        0.18704237,  0.53531992,  0.38024127,  0.17129602, -0.56195861,\n",
       "       -0.06693541,  0.0627511 , -0.32564688,  0.00486778, -0.11741938,\n",
       "        0.41015202, -0.6087113 , -0.06891805, -0.28068548,  0.45937932,\n",
       "       -0.0275434 , -0.25169009, -0.07305765, -0.44404814,  0.21929997], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs[example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Perceptual Tuple consists of the values for 100 *Perceptual Features*. They are implicit, so we cannot understand or intepret their meaning.\n",
    "\n",
    "It would be awesome if after training we could check the values for reviews that mention great action and find what PFs they have in common. Or reviews that talk about great humor and compare their tuples to the ones saying that the movie was boring, and find if there are attributes high for the first and low for the second.\n",
    "\n",
    "Sadly, if the third value is -0.25758761 or 0.32343212, it makes no sense to us humans.\n",
    "\n",
    "Anyway, we can start by testing the similarity of a document with itself, as a naive sanity check. First by using the similarity measure implemented by Gensim (should be 1), second by computing the cosine distance between the vectors using scipy (should be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs.similarity(d1='ADZPIG9QOCDG5|0005019281', d2='ADZPIG9QOCDG5|0005019281'))\n",
    "\n",
    "print(round(spatial.distance.cosine(model.docvecs['ADZPIG9QOCDG5|0005019281'], model.docvecs['ADZPIG9QOCDG5|0005019281']), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can input a given ID and obtain the most similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top  similar reviews to: I was dressed up at my school as freddy krueger yesterday (for my halloween dance) and then i watched this movie. Theis has to be one of the best of the Nightmare. My favorite part is wehn the Freddipilliar appears and eats the little girl. Its awesome. WATCH THIS AWESOME SH**.\n",
      "ID:  A39W3263A9HCMN|0780630866  Review:  I was dressed up at my school as freddy krueger yesterday (for my halloween dance) and then i watched this movie. Theis has to be one of the best of the Nightmare. My favorite part is wehn the Freddipilliar appears and eats the little girl. Its awesome. WATCH THIS AWESOME SH**.\n",
      "\n",
      "ID:  A3DEO4BBK4TQ1Q|0780619412  Review:  THIS IS ON MY TOP TEN FAVORITE MOVIES OF ALL TIME. THE FIST TIME I SAW IT I DIDN'T SLEEP FOR A WEEK. FREDDY WITHOUT HUMOR IS SCARY. DEFINITLY THE BEST NIGHTMARE FILM EVER!\n",
      "\n",
      "ID:  A2XU709F7V64T|0780619412  Review:  ok,i aint a big fan of freddy's movies,but id say this is the best one and scariest,i liked part 3 alot too,check this movie out.\n",
      "\n",
      "ID:  A3VHYPCUXD7VHT|0780630866  Review:  Freddy Krueger is my favorite horror icon of all time! He is fun, and scary, but not too over the top when it comes to gore. My personal favorites are Nightmare 1, Nightmare 3: Dream Warriors, Wes Craven's New Nightmare, & Freddy vs. Jason. But, anything with Freddy in it is AWESOME!!! Highly recommended!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims = model.docvecs.most_similar(positive=[model.docvecs[example]], topn=4)\n",
    "\n",
    "print('Top  similar reviews to:', review_text[example])\n",
    "for review in sims:\n",
    "    print('ID: ', review[0], ' Review: ', review_text[review[0]]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find only the most similar reviews for the same item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_product_similars(review_id, number):    \n",
    "    sims = model.docvecs.most_similar(positive=[review_id], topn=100)\n",
    "    print('Top similar reviews to: ', review_text[review_id]+'\\n')\n",
    "    i = 0\n",
    "    for sim in sims:\n",
    "        s = sim[0]\n",
    "        asin = s[(s.index('|'))+1:]\n",
    "        if i < number:\n",
    "            if asin == review_id[(review_id.index('|'))+1:]:\n",
    "                print('ID: ', sim[0], ' Review: ', review_text[sim[0]]+'\\n')\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar reviews to:  I was dressed up at my school as freddy krueger yesterday (for my halloween dance) and then i watched this movie. Theis has to be one of the best of the Nightmare. My favorite part is wehn the Freddipilliar appears and eats the little girl. Its awesome. WATCH THIS AWESOME SH**.\n",
      "\n",
      "ID:  A3VHYPCUXD7VHT|0780630866  Review:  Freddy Krueger is my favorite horror icon of all time! He is fun, and scary, but not too over the top when it comes to gore. My personal favorites are Nightmare 1, Nightmare 3: Dream Warriors, Wes Craven's New Nightmare, & Freddy vs. Jason. But, anything with Freddy in it is AWESOME!!! Highly recommended!\n",
      "\n",
      "ID:  A2940X5L71GK3U|0780630866  Review:  I like scary movies (Horror, Thriller, Suspense). This may be one of the best horror movies ever. This was cool and it was one of four scary movies  that actually scared me. The other ones where Thinner, The Shinning, and  The Sixth Sense. Out of all the Halloween movies, Friday the 13ths, and  Nightmare on Elm Streets, this is the best one to get. Although its got  that 80's cheesieness, it only really comes out about the 7th time you  watch it. Its got those cool effects to. Like Freddy Krueger becomes a worm  in a dream of this girl's when she is in the doll house.\n",
      "\n",
      "ID:  A260HDJ5Z8CUX2|0780630866  Review:  The delivery time was in perfect time. I love all the Freddy's movies and this is really scary, I think this is the best part with the children an the psychiatric !\n",
      "\n",
      "ID:  AB4ZGWRDU7VRB|0780630866  Review:  this is the best sequel nancy and her dad are back wes craven is back freddy is back to actually kill people in there dreams again hes even funny now and has one of the best and most famous killings and its really fun so it is way better than part two this is the best sequel\n",
      "\n",
      "ID:  A1CDZNPO7U4E6N|0780630866  Review:  I am a big \"Nightmare on Elm Street\" fan, and this movie exceeded my expectations. I was thinking it was just going to be another bad sequel but, on the contrary this movie is good. Robert Englund does once again an outstanding job at being Freddie Kruegger. I was actually kind of jumpy when I was watching this movie, and that's unusual for me.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "same_product_similars('A39W3263A9HCMN|0780630866', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also allows to infer the vector of a given sentence (separated by words like the TaggedDocuments above) and test how similar it is to others. This is important to get vectors from new sentences that were not in the training set for the model. In this case we try it with a sentence that *is* in the dataset, to see which ones are the most similar. The first sentence *should* be itself.\n",
    "\n",
    "Higher numbers of steps or iterations in the inference process will achieve a better similarity score to itself, while reducing it to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top  similar reviews to: I was dressed up at my school as freddy krueger yesterday (for my halloween dance) and then i watched this movie. Theis has to be one of the best of the Nightmare. My favorite part is wehn the Freddipilliar appears and eats the little girl. Its awesome. WATCH THIS AWESOME SH**.\n",
      "\n",
      "ID:  A39W3263A9HCMN|0780630866  Review:  I was dressed up at my school as freddy krueger yesterday (for my halloween dance) and then i watched this movie. Theis has to be one of the best of the Nightmare. My favorite part is wehn the Freddipilliar appears and eats the little girl. Its awesome. WATCH THIS AWESOME SH**.\n",
      "\n",
      "ID:  A1M82OE9TB0RQ0|0780630874  Review:  Ok. This is like, the best of the Freddy flicks. Dream Child used to be the best to me, then I started watchin this one a lot more. The F/X are great! The cockroach scene is kool. And the song while the main titles are rolling is pretty kool, too. And Lisa Zane is the best female survivor I've seen in the more major horror flicks (including HALLOWEEN, FRIDAY THE 13TH, A NIGHTMARE ON ELM STREET).\n",
      "\n",
      "ID:  A3DEO4BBK4TQ1Q|0780619412  Review:  THIS IS ON MY TOP TEN FAVORITE MOVIES OF ALL TIME. THE FIST TIME I SAW IT I DIDN'T SLEEP FOR A WEEK. FREDDY WITHOUT HUMOR IS SCARY. DEFINITLY THE BEST NIGHTMARE FILM EVER!\n",
      "\n",
      "ID:  A2JVZQFRQM79M|0780626966  Review:  All your favorite killer nightmare man in one boxed set. I myself, I prefer the 3rd movie over all the others. The return of Nancy (she dies of course) but yes, the killer nightmare franchise strikes again. With a boxes set!\n",
      "\n",
      "ID:  A260HDJ5Z8CUX2|0780630866  Review:  The delivery time was in perfect time. I love all the Freddy's movies and this is really scary, I think this is the best part with the children an the psychiatric !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_docvec = model.infer_vector(word_tokenize(review_text[example]), steps=5000, alpha = 0.01)\n",
    "sims = model.docvecs.most_similar(positive=[inferred_docvec], topn=5)\n",
    "\n",
    "print('Top  similar reviews to:', review_text[example]+'\\n')\n",
    "for review in sims:\n",
    "    print('ID: ', review[0], ' Review: ', review_text[review[0]]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the cosine distance between the vector and its own word-based inference.\n",
    "We can experiment with some values for the number of steps or alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.1 0.1\n",
      "100 0.01 0.07\n",
      "100 0.001 0.14\n",
      "1000 0.1 0.05\n",
      "1000 0.01 0.02\n",
      "1000 0.001 0.02\n",
      "10000 0.1 0.09\n",
      "10000 0.01 0.09\n",
      "10000 0.001 0.09\n"
     ]
    }
   ],
   "source": [
    "n_steps = [100, 1000, 10000]\n",
    "alphas = [0.1, 0.01, 0.001]\n",
    "\n",
    "for trial in n_steps:\n",
    "    for alpha_value in alphas:\n",
    "        inferred_docvec = model.infer_vector(word_tokenize(review_text[example].lower()), steps = trial, alpha = alpha_value) \n",
    "        print(trial, alpha_value, round(spatial.distance.cosine(inferred_docvec, model.docvecs[example]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 1000 steps with alpha = 0.01 yield the most similar vectors, so we could use this in future parts of the process, if necessary.\n",
    "\n",
    "Now we have a trained doc2vec model on movie reviews, and we can evaluate similarity of the review texts, find the most similar ones, etc.\n",
    "\n",
    "The next step is to group similar perceptual tuples into Shared Perspectives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
